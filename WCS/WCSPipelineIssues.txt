Although it is architected from the proper domain objects, the current
WCS prototype is a poor guide to how the real LSST pipeline will be
making use of computational resources.  There are two major ways in
which the prototype needs to be extended to be a more realistic and
useful model:

   o Use of parallel computers

   o Use of memory for transmitting objects between pipeline stages

1. Use of parallel computers
-----------------------------

The computation of the WCS for a large mosaic imager is to first order
embarassingly parallel.  Each ccd that makes up the mosaic can be
independently processed.  This is reflected in the test9.py test
program, that explicitly loops over all the component ccds.  The
situation is more complex than it first appears, however, for one big
reason and a number of smaller ones.  The big reason is that it seems
clear that the basic algorithm will need to be extended to include a
piece that simultaneously optimizes all the individual WCS's that make
up the mosaic.  This will take the form of the minimization of a
nonlinear function of roughly 600 - 1200 variables (three to six per
ccd to describe their orientation, plus 10 or so to describe the
optical distortion of the corrector and atmospheric refraction). This
optimization stage can certainly be made at least partially parallel,
but will require significant communication between the parallel
threads.   This will lead us to use MPI or similar, and the
implementation of this stage may well end up dominating the
performance picture.

There are also a number of smaller implementation details that need to
be resolved before the current pipeline will work properly in a
parallel environment.  One area that needs attention is the operation
of sextractor.  At present, sextractor consumes and produces a number
of files (see section 2 below, as well), and there is currently no
code in place to make the filenames unique to a CCD.   Care also needs
to be taken in writing the individual CCD WCS information back into
the parent MEF file.   PyFITS is very unlikely to be thread safe, so
this operation must be serialized.   Doubtless there are other similar issues.

2. Use of memory for transmitting objects between pipeline stages
-----------------------------------------------------------------

The current prototype at several points makes use of files to transmit
objects between pipeline stages.  For example, the individual ccd
images are written to a fits file before they are read in by
sextractor.  In the full scale system, we will not be able to achieve
our performance goals with this level of disk I/O.  In one way or
another, pipeline data must move through memory.   The easiest way out
is certainly to set up ramdisks, which is easy to do under linux.
This may be a reasonable first step, but may not be what we actually
want in our middleware.  This area requires some thought and experimentation.
